{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS444 Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from kaggle_submission import output_submission_csv\n",
    "from models.neural_net import NeuralNetwork\n",
    "from utils.data_process import get_FASHION_data\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # set default size of plots\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fashion-MNIST\n",
    "Now that you have implemented a neural network that passes gradient checks and works on toy data, you will test your network on the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission be sure they are set to the default values\n",
    "TRAIN_IMAGES = 50000\n",
    "VAL_IMAGES = 10000\n",
    "TEST_IMAGES = 10000\n",
    "\n",
    "data = get_FASHION_data(TRAIN_IMAGES, VAL_IMAGES, TEST_IMAGES)\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_val, y_val = data['X_val'], data['y_val']\n",
    "X_test, y_test = data['X_test'], data['y_test']\n",
    "\n",
    "train_loss = None\n",
    "train_accuracy = None\n",
    "val_accuracy = None\n",
    "net_2sgd, net_3sgd, net_2adam, net_3adam = None, None, None, None\n",
    "\n",
    "sgd2_loss, sgd2_val, adam2_loss, adam2_val = None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using SGD\n",
    "To train our network we will use SGD. In addition, we will adjust the learning rate with an exponential learning rate schedule as optimization proceeds; after each epoch, we will reduce the learning rate by multiplying it by a decay rate.\n",
    "\n",
    "You can try different numbers of layers and other hyperparameters on the Fashion-MNIST dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 1/50 [02:07<1:44:00, 127.36s/it, valid=0.8117, train=0.7721]"
     ]
    }
   ],
   "source": [
    "def sgd_2():\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = 28 * 28\n",
    "    num_layers = 2\n",
    "    hidden_size = 80\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 50\n",
    "    batch_size = 100\n",
    "    learning_rate = 1e-3\n",
    "    learning_rate_decay = 0.8\n",
    "    regularization = 0.08\n",
    "\n",
    "    global train_loss, train_accuracy, val_accuracy, net_2sgd\n",
    "    # Initialize a new neural network model\n",
    "    net_2sgd = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "    # Variables to store performance for each epoch\n",
    "    train_loss = np.zeros(epochs)\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    val_accuracy = np.zeros(epochs)\n",
    "\n",
    "    # For each epoch...\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X, y = X_train[perm], y_train[perm]\n",
    "\n",
    "        # Training\n",
    "        # For each mini-batch...\n",
    "        batches = TRAIN_IMAGES // batch_size\n",
    "        for batch in range(batches):\n",
    "            # Create a mini-batch of training data and labels\n",
    "            X_batch = X[batch*batch_size:(batch + 1)*batch_size]\n",
    "            y_batch = y[batch*batch_size:(batch + 1)*batch_size]\n",
    "\n",
    "            # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "            train_accuracy[epoch] += np.sum(\n",
    "                np.argmax(net_2sgd.forward(X_batch), axis=1) == y_batch)\n",
    "\n",
    "            # Run the backward pass of the model to compute the loss, and update the weights\n",
    "            train_loss[epoch] += net_2sgd.backward(y_batch, regularization)\n",
    "            net_2sgd.update(learning_rate, opt='SGD')\n",
    "\n",
    "        # Validation\n",
    "        # No need to run the backward pass here, just run the forward pass to compute accuracy\n",
    "        val_accuracy[epoch] += np.sum(np.argmax(net_2sgd.forward(X_val),\n",
    "                                    axis=1) == y_val) / y_val.shape[0]\n",
    "        t.set_postfix({\"valid\":\"{:.4f}\".format(val_accuracy[epoch]),\n",
    "                        \"train\": \"{:.4f}\".format(train_accuracy[epoch] / (batches * batch_size))})\n",
    "        # Implement learning rate decay\n",
    "        learning_rate = learning_rate * learning_rate_decay\n",
    "        train_accuracy[epoch] = train_accuracy[epoch] / (batches * batch_size)\n",
    "        train_loss[epoch] = train_loss[epoch] / (batches * batch_size)\n",
    "    global sgd2_loss, sgd2_val\n",
    "    sgd2_loss = train_loss.copy()\n",
    "    sgd2_val = val_accuracy.copy()\n",
    "sgd_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_3():\n",
    "    # Hyperparameters\n",
    "    input_size = 28 * 28\n",
    "    num_layers = 3\n",
    "    hidden_size = 80\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 40\n",
    "    batch_size = 50\n",
    "    learning_rate = 0.0005\n",
    "    learning_rate_decay = 0.95\n",
    "    regularization = 0.08\n",
    "\n",
    "    global train_loss, train_accuracy, val_accuracy, net_3sgd\n",
    "    # Initialize a new neural network model\n",
    "    net_3sgd = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "    # Variables to store performance for each epoch\n",
    "\n",
    "    train_loss = np.zeros(epochs)\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    val_accuracy = np.zeros(epochs)\n",
    "\n",
    "    # For each epoch...\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X, y = X_train[perm], y_train[perm]\n",
    "\n",
    "        # Training\n",
    "        # For each mini-batch...\n",
    "        batches = TRAIN_IMAGES // batch_size\n",
    "        for batch in range(batches):\n",
    "            # Create a mini-batch of training data and labels\n",
    "            X_batch = X[batch*batch_size:(batch + 1)*batch_size]\n",
    "            y_batch = y[batch*batch_size:(batch + 1)*batch_size]\n",
    "\n",
    "            # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "            train_accuracy[epoch] += np.sum(\n",
    "                np.argmax(net_3sgd.forward(X_batch), axis=1) == y_batch)\n",
    "\n",
    "            # Run the backward pass of the model to compute the loss, and update the weights\n",
    "            train_loss[epoch] += net_3sgd.backward(y_batch, regularization)\n",
    "            net_3sgd.update(learning_rate, opt='SGD')\n",
    "\n",
    "        # Validation\n",
    "        # No need to run the backward pass here, just run the forward pass to compute accuracy\n",
    "        val_accuracy[epoch] += np.sum(np.argmax(net_3sgd.forward(X_val),\n",
    "                                    axis=1) == y_val) / y_val.shape[0]\n",
    "        t.set_postfix({\"valid\":\"{:.4f}\".format(val_accuracy[epoch]),\n",
    "                        \"train\": \"{:.4f}\".format(train_accuracy[epoch] / (batches * batch_size))})\n",
    "        # Implement learning rate decay\n",
    "        learning_rate = learning_rate * learning_rate_decay\n",
    "        train_accuracy[epoch] = train_accuracy[epoch] / (batches * batch_size)\n",
    "        train_loss[epoch] = train_loss[epoch] / (batches * batch_size)\n",
    "\n",
    "# sgd_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using Adam\n",
    "Next we will train the same model using the Adam optimizer. You should take the above code for SGD and modify it to use Adam instead. For implementation details, see the lecture slides. The original paper that introduced Adam is also a good reference, and contains suggestions for default values: https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_2():\n",
    "    # Hyperparameters\n",
    "    input_size = 28 * 28\n",
    "    num_layers = 2\n",
    "    hidden_size = 80\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 50\n",
    "    batch_size = 50\n",
    "    learning_rate = 1e-4\n",
    "    regularization = 0.12\n",
    "    learning_rate_decay = 0.95\n",
    "\n",
    "\n",
    "    global train_loss, train_accuracy, val_accuracy, net_2adam\n",
    "    # Initialize a new neural network model\n",
    "    net_2adam = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "    # Variables to store performance for each epoch\n",
    "    train_loss = np.zeros(epochs)\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    val_accuracy = np.zeros(epochs)\n",
    "\n",
    "    # For each epoch...\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X, y = X_train[perm], y_train[perm]\n",
    "\n",
    "        # Training\n",
    "        # For each mini-batch...\n",
    "        batches = TRAIN_IMAGES // batch_size\n",
    "        for batch in range(batches):\n",
    "            # Create a mini-batch of training data and labels\n",
    "            X_batch = X[batch*batch_size:(batch + 1)*batch_size]\n",
    "            y_batch = y[batch*batch_size:(batch + 1)*batch_size]\n",
    "\n",
    "            # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "            train_accuracy[epoch] += np.sum(\n",
    "                np.argmax(net_2adam.forward(X_batch), axis=1) == y_batch)\n",
    "\n",
    "            # Run the backward pass of the model to compute the loss, and update the weights\n",
    "            train_loss[epoch] += net_2adam.backward(y_batch, regularization)\n",
    "            net_2adam.update(learning_rate, opt='Adam')\n",
    "\n",
    "        # Validation\n",
    "        # No need to run the backward pass here, just run the forward pass to compute accuracy\n",
    "        val_accuracy[epoch] += np.sum(np.argmax(net_2adam.forward(X_val),\n",
    "                                    axis=1) == y_val)\n",
    "        t.set_postfix({\"valid\":\"{:.4f}\".format(val_accuracy[epoch] / y_val.shape[0]),\n",
    "                        \"train\": \"{:.4f}\".format(train_accuracy[epoch] / (batches * batch_size))})\n",
    "        # Implement learning rate decay\n",
    "        train_accuracy[epoch] = train_accuracy[epoch] / (batches * batch_size)\n",
    "        train_loss[epoch] = train_loss[epoch] / (batches * batch_size)\n",
    "        learning_rate = learning_rate * learning_rate_decay\n",
    "    global adam2_loss, adam2_val\n",
    "    adam2_loss = train_loss.copy()\n",
    "    adam2_val = val_accuracy.copy()\n",
    "# adam_2()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [40:52<00:00, 49.05s/it, valid=0.8901, train=0.9367]\n"
     ]
    }
   ],
   "source": [
    "def adam_3():\n",
    "    # Hyperparameters\n",
    "    input_size = 28 * 28\n",
    "    num_layers = 3\n",
    "    hidden_size = 60\n",
    "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
    "    num_classes = 10\n",
    "    epochs = 50\n",
    "    batch_size = 50\n",
    "    learning_rate = 0.005\n",
    "    regularization = 0.03\n",
    "    learning_rate_decay = 0.95\n",
    "\n",
    "\n",
    "    global train_loss, train_accuracy, val_accuracy, net_3adam\n",
    "    # Initialize a new neural network model\n",
    "    net_3adam = NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers)\n",
    "\n",
    "    # Variables to store performance for each epoch\n",
    "    train_loss = np.zeros(epochs)\n",
    "    train_accuracy = np.zeros(epochs)\n",
    "    val_accuracy = np.zeros(epochs)\n",
    "\n",
    "    # For each epoch...\n",
    "    t = tqdm(range(epochs))\n",
    "    for epoch in t:\n",
    "\n",
    "        # Shuffle the dataset\n",
    "        perm = np.random.permutation(X_train.shape[0])\n",
    "        X, y = X_train[perm], y_train[perm]\n",
    "\n",
    "        # Training\n",
    "        # For each mini-batch...\n",
    "        batches = TRAIN_IMAGES // batch_size\n",
    "        for batch in range(batches):\n",
    "            # Create a mini-batch of training data and labels\n",
    "            X_batch = X[batch*batch_size:(batch + 1)*batch_size]\n",
    "            y_batch = y[batch*batch_size:(batch + 1)*batch_size]\n",
    "\n",
    "            # Run the forward pass of the model to get a prediction and compute the accuracy\n",
    "            train_accuracy[epoch] += np.sum(\n",
    "                np.argmax(net_3adam.forward(X_batch), axis=1) == y_batch)\n",
    "\n",
    "            # Run the backward pass of the model to compute the loss, and update the weights\n",
    "            train_loss[epoch] += net_3adam.backward(y_batch, regularization)\n",
    "            net_3adam.update(learning_rate, 0.9, 0.99, opt='SGD')\n",
    "\n",
    "        # Validation\n",
    "        # No need to run the backward pass here, just run the forward pass to compute accuracy\n",
    "        val_accuracy[epoch] += np.sum(np.argmax(net_3adam.forward(X_val),\n",
    "                                    axis=1) == y_val)\n",
    "        t.set_postfix({\"valid\":\"{:.4f}\".format(val_accuracy[epoch] / y_val.shape[0]),\n",
    "                        \"train\": \"{:.4f}\".format(train_accuracy[epoch] / (batches * batch_size))})\n",
    "        # Implement learning rate decay\n",
    "        train_accuracy[epoch] = train_accuracy[epoch] / (batches * batch_size)\n",
    "        train_loss[epoch] = train_loss[epoch] / (batches * batch_size)\n",
    "        learning_rate = learning_rate * learning_rate_decay\n",
    "# adam_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph loss and train/val accuracies\n",
    "\n",
    "Examining the loss graph along with the train and val accuracy graphs should help you gain some intuition for the hyperparameters you should try in the hyperparameter tuning below. It should also help with debugging any issues you might have with your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(train_loss)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_accuracy, label='train')\n",
    "plt.plot(val_accuracy, label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "Once you have successfully trained a network you can tune your hyparameters to increase your accuracy.\n",
    "\n",
    "Based on the graphs of the loss function above you should be able to develop some intuition about what hyperparameter adjustments may be necessary. A very noisy loss implies that the learning rate might be too high, while a linearly decreasing loss would suggest that the learning rate may be too low. A large gap between training and validation accuracy would suggest overfitting due to a large model without much regularization. No gap between training and validation accuracy would indicate low model capacity. \n",
    "\n",
    "You will compare networks of two and three layers using the different optimization methods you implemented. \n",
    "\n",
    "The different hyperparameters you can experiment with are:\n",
    "- **Batch size**: We recommend you leave this at 200 initially which is the batch size we used. \n",
    "- **Number of iterations**: You can gain an intuition for how many iterations to run by checking when the validation accuracy plateaus in your train/val accuracy graph.\n",
    "- **Initialization** Weight initialization is very important for neural networks. We used the initialization `W = np.random.randn(n) / sqrt(n)` where `n` is the input dimension for layer corresponding to `W`. We recommend you stick with the given initializations, but you may explore modifying these. Typical initialization practices: http://cs231n.github.io/neural-networks-2/#init\n",
    "- **Learning rate**: Generally from around 1e-4 to 1e-1 is a good range to explore according to our implementation.\n",
    "- **Learning rate decay**: We recommend a 0.95 decay to start.\n",
    "- **Hidden layer size**: You should explore up to around 120 units per layer. For three-layer network, we fixed the two hidden layers to be the same size when obtaining the target numbers. However, you may experiment with having different size hidden layers.\n",
    "- **Regularization coefficient**: We recommend trying values in the range 0 to 0.1. \n",
    "\n",
    "Hints:\n",
    "- After getting a sense of the parameters by trying a few values yourself, you will likely want to write a few for-loops to traverse over a set of hyperparameters.\n",
    "- If you find that your train loss is decreasing, but your train and val accuracy start to decrease rather than increase, your model likely started minimizing the regularization term. To prevent this you will need to decrease the regularization coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on the test set\n",
    "When you are done experimenting, you should evaluate your final trained networks on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_2layer_sgd_prediction = np.argmax(net_2sgd.forward(X_test), axis=1)\n",
    "# best_3layer_sgd_prediction = np.argmax(net_3sgd.forward(X_test), axis=1)\n",
    "# best_2layer_adam_prediction = np.argmax(net_2adam.forward(X_test), axis=1)\n",
    "best_3layer_adam_prediction = np.argmax(net_3adam.forward(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle output\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 2 Neural Network. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_submission_csv('./nn_2layer_sgd_submission.csv', best_2layer_sgd_prediction)\n",
    "# output_submission_csv('./nn_3layer_sgd_submission.csv', best_3layer_sgd_prediction)\n",
    "# output_submission_csv('./nn_2layer_adam_submission.csv', best_2layer_adam_prediction)\n",
    "output_submission_csv('./nn_3layer_adam_submission.csv', best_3layer_adam_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare SGD and Adam\n",
    "Create graphs to compare training loss and validation accuracy between SGD and Adam. The code is similar to the above code, but instead of comparing train and validation, we are comparing SGD and Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x, y, and format string must not be None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16061/2050711889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the loss function and train / validation accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madam2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msgd2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hdd/miniconda3/envs/py10/lib/python3.10/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2756\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2757\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2758\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2759\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/hdd/miniconda3/envs/py10/lib/python3.10/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1630\u001b[0m         \"\"\"\n\u001b[1;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1632\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1633\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hdd/miniconda3/envs/py10/lib/python3.10/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hdd/miniconda3/envs/py10/lib/python3.10/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;31m# element array of None which causes problems downstream.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x, y, and format string must not be None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x, y, and format string must not be None"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlsAAADpCAYAAAATWPImAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOEUlEQVR4nO3df6jdd33H8dfbxk6o1cGSgTSJLSydZirUXboO/7BgN9L+kfzhJi2IU4r5ZxU3RagoVepfKnMg1B8Zk05Bu+gfcsFIB65SECNN6VZMSyVEZ1OFVq39p2jt9t4f5yjX2yT329vzuclJHw8I3O/3fO45b/hwk2e+59xzqrsDAMAYLznXAwAAXMjEFgDAQGILAGAgsQUAMJDYAgAYSGwBAAy0YWxV1Req6vGq+v4Zbq+q+nRVnaiqB6vqjYsfEwBgOU25snVnkn1nuf36JHvmfw4m+ewLHwsA4MKwYWx1971JfnGWJQeSfLFnjib5w6p61aIGBABYZot4zdZlSR5dc3xqfg4A4EVv21Y+WFUdzOypxlxyySV//prXvGYrHx4AYFPuv//+n3X3js187yJi67Eku9Yc75yfe47uPpTkUJKsrKz0sWPHFvDwAABjVdX/bPZ7F/E04mqSd8x/K/GaJE91908XcL8AAEtvwytbVfWVJNcm2V5Vp5J8JMlLk6S7P5fkSJIbkpxI8nSSd40aFgBg2WwYW9190wa3d5K/X9hEAAAXEO8gDwAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAk2KrqvZV1SNVdaKqbj3N7bur6p6qeqCqHqyqGxY/KgDA8tkwtqrqoiR3JLk+yd4kN1XV3nXLPpzkcHdfleTGJJ9Z9KAAAMtoypWtq5Oc6O6T3f1MkruSHFi3ppO8Yv71K5P8ZHEjAgAsr20T1lyW5NE1x6eS/MW6NR9N8h9V9Z4klyS5biHTAQAsuUW9QP6mJHd2984kNyT5UlU9576r6mBVHauqY0888cSCHhoA4Pw1JbYeS7JrzfHO+bm1bk5yOEm6+7tJXpZk+/o76u5D3b3S3Ss7duzY3MQAAEtkSmzdl2RPVV1RVRdn9gL41XVrfpzkLUlSVa/NLLZcugIAXvQ2jK3ufjbJLUnuTvJwZr91eLyqbq+q/fNl70/y7qr67yRfSfLO7u5RQwMALIspL5BPdx9JcmTdudvWfP1QkjctdjQAgOXnHeQBAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADDQpNiqqn1V9UhVnaiqW8+w5m1V9VBVHa+qLy92TACA5bRtowVVdVGSO5L8VZJTSe6rqtXufmjNmj1JPpjkTd39ZFX98aiBAQCWyZQrW1cnOdHdJ7v7mSR3JTmwbs27k9zR3U8mSXc/vtgxAQCW05TYuizJo2uOT83PrXVlkiur6jtVdbSq9i1qQACAZbbh04jP4372JLk2yc4k91bV67v7l2sXVdXBJAeTZPfu3Qt6aACA89eUK1uPJdm15njn/Nxap5KsdvdvuvuHSX6QWXz9nu4+1N0r3b2yY8eOzc4MALA0psTWfUn2VNUVVXVxkhuTrK5b8/XMrmqlqrZn9rTiycWNCQCwnDaMre5+NsktSe5O8nCSw919vKpur6r982V3J/l5VT2U5J4kH+jun48aGgBgWVR3n5MHXllZ6WPHjp2TxwYAeD6q6v7uXtnM93oHeQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGmhRbVbWvqh6pqhNVdetZ1r21qrqqVhY3IgDA8towtqrqoiR3JLk+yd4kN1XV3tOsuzTJe5N8b9FDAgAsqylXtq5OcqK7T3b3M0nuSnLgNOs+luTjSX61wPkAAJbalNi6LMmja45Pzc/9TlW9Mcmu7v7GAmcDAFh6L/gF8lX1kiSfSvL+CWsPVtWxqjr2xBNPvNCHBgA4702JrceS7FpzvHN+7rcuTfK6JN+uqh8luSbJ6uleJN/dh7p7pbtXduzYsfmpAQCWxJTYui/Jnqq6oqouTnJjktXf3tjdT3X39u6+vLsvT3I0yf7uPjZkYgCAJbJhbHX3s0luSXJ3koeTHO7u41V1e1XtHz0gAMAy2zZlUXcfSXJk3bnbzrD22hc+FgDAhcE7yAMADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYCCxBQAwkNgCABhIbAEADCS2AAAGElsAAAOJLQCAgcQWAMBAYgsAYKBJsVVV+6rqkao6UVW3nub291XVQ1X1YFV9q6pevfhRAQCWz4axVVUXJbkjyfVJ9ia5qar2rlv2QJKV7n5Dkq8l+cSiBwUAWEZTrmxdneREd5/s7meS3JXkwNoF3X1Pdz89PzyaZOdixwQAWE5TYuuyJI+uOT41P3cmNyf55gsZCgDgQrFtkXdWVW9PspLkzWe4/WCSg0mye/fuRT40AMB5acqVrceS7FpzvHN+7vdU1XVJPpRkf3f/+nR31N2Hunulu1d27NixmXkBAJbKlNi6L8meqrqiqi5OcmOS1bULquqqJJ/PLLQeX/yYAADLacPY6u5nk9yS5O4kDyc53N3Hq+r2qto/X/bJJC9P8tWq+q+qWj3D3QEAvKhMes1Wdx9JcmTdudvWfH3dgucCALggeAd5AICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAMJLYAAAaaFFtVta+qHqmqE1V162lu/4Oq+vf57d+rqssXPikAwBLaMLaq6qIkdyS5PsneJDdV1d51y25O8mR3/0mSf07y8UUPCgCwjKZc2bo6yYnuPtndzyS5K8mBdWsOJPm3+ddfS/KWqqrFjQkAsJymxNZlSR5dc3xqfu60a7r72SRPJfmjRQwIALDMtm3lg1XVwSQH54e/rqrvb+Xjs1Dbk/zsXA/Bpti75Wb/lpe9W25/utlvnBJbjyXZteZ45/zc6dacqqptSV6Z5Ofr76i7DyU5lCRVday7VzYzNOee/Vte9m652b/lZe+WW1Ud2+z3Tnka8b4ke6rqiqq6OMmNSVbXrVlN8nfzr/8myX92d292KACAC8WGV7a6+9mquiXJ3UkuSvKF7j5eVbcnOdbdq0n+NcmXqupEkl9kFmQAAC96k16z1d1HkhxZd+62NV//KsnfPs/HPvQ813N+sX/Ly94tN/u3vOzdctv0/pVn+wAAxvFxPQAAAw2PLR/1s7wm7N37quqhqnqwqr5VVa8+F3Nyehvt35p1b62qriq/JXUembJ/VfW2+c/g8ar68lbPyOlN+Ltzd1XdU1UPzP/+vOFczMlzVdUXqurxM701Vc18er63D1bVG6fc79DY8lE/y2vi3j2QZKW735DZJwd8Ymun5Ewm7l+q6tIk703yva2dkLOZsn9VtSfJB5O8qbv/LMk/bPWcPNfEn70PJznc3Vdl9gtln9naKTmLO5PsO8vt1yfZM/9zMMlnp9zp6CtbPupneW24d919T3c/PT88mtl7sHF+mPKzlyQfy+w/OL/ayuHY0JT9e3eSO7r7ySTp7se3eEZOb8redZJXzL9+ZZKfbOF8nEV335vZuyqcyYEkX+yZo0n+sKpetdH9jo4tH/WzvKbs3Vo3J/nm0Il4Pjbcv/nl713d/Y2tHIxJpvz8XZnkyqr6TlUdraqz/W+crTNl7z6a5O1VdSqz3/R/z9aMxgI8338bk2zxx/VwYaqqtydZSfLmcz0L01TVS5J8Ksk7z/EobN62zJ7KuDazq8r3VtXru/uX53IoJrkpyZ3d/U9V9ZeZvU/l67r7/871YIwx+srW8/mon5zto37YclP2LlV1XZIPJdnf3b/eotnY2Eb7d2mS1yX5dlX9KMk1SVa9SP68MeXn71SS1e7+TXf/MMkPMosvzq0pe3dzksNJ0t3fTfKyzD43kfPfpH8b1xsdWz7qZ3ltuHdVdVWSz2cWWl4vcn456/5191Pdvb27L+/uyzN7zd3+7t70Z3+xUFP+7vx6Zle1UlXbM3ta8eQWzsjpTdm7Hyd5S5JU1Wszi60ntnRKNms1yTvmv5V4TZKnuvunG33T0KcRfdTP8pq4d59M8vIkX53/TsOPu3v/ORua35m4f5ynJu7f3Un+uqoeSvK/ST7Q3Z4VOMcm7t37k/xLVf1jZi+Wf6eLDOeHqvpKZv+J2T5/Td1Hkrw0Sbr7c5m9xu6GJCeSPJ3kXZPu1/4CAIzjHeQBAAYSWwAAA4ktAICBxBYAwEBiCwBgILEFADCQ2AIAGEhsAQAM9P98QquOrdkz5QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: implement me\n",
    "\n",
    "# Plot the loss function and train / validation accuracies\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(adam2_loss, label=\"adam\")\n",
    "plt.plot(sgd2_loss, label=\"sgd\")\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(adam2_val, label='adam')\n",
    "plt.plot(sgd2_val, label='sgd')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Classification accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
